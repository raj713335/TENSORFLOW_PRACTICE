{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zX4Kg8DUTKWO"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-N3Pz2T9mFZ-"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "5p90mFICmFK2"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0IMg_z16l_2Z"
   },
   "source": [
    "# Text Classification\n",
    "\n",
    "In this notebook we will classify movie reviews as being either `positive` or `negative`. We'll use the [IMDB dataset](https://www.tensorflow.org/datasets/catalog/imdb_reviews) that contains the text of 50,000 movie reviews from the [Internet Movie Database](https://www.imdb.com/). These are split into 25,000 reviews for training and 25,000 reviews for testing. The training and testing sets are *balanced*, meaning they contain an equal number of positive and negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XNQOOgA5xiKa"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/TensorFlow%20Deployment/Course%204%20-%20TensorFlow%20Serving/Week%202/Examples/text_classification.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/lmoroney/dlaicourse/blob/master/TensorFlow%20Deployment/Course%204%20-%20TensorFlow%20Serving/Week%202/Examples/text_classification.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l3JBInnkl_2a"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "02pc7RECl_2b",
    "outputId": "d4c21603-299f-41fc-a3f8-4c77170d5c38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_datasets\n",
      "  Using cached tensorflow_datasets-3.1.0-py3-none-any.whl (3.3 MB)\n",
      "Requirement already satisfied: wrapt in c:\\users\\zeus\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (1.11.2)\n",
      "Requirement already satisfied: absl-py in c:\\users\\zeus\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (0.9.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\zeus\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (2.22.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\zeus\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (1.18.1)\n",
      "Requirement already satisfied: six in c:\\users\\zeus\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (1.14.0)\n",
      "Requirement already satisfied: termcolor in c:\\users\\zeus\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (1.1.0)\n",
      "Collecting tensorflow-metadata\n",
      "  Using cached tensorflow_metadata-0.22.2-py2.py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: future in c:\\users\\zeus\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (0.18.2)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in c:\\users\\zeus\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (3.12.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\zeus\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (4.42.1)\n",
      "Requirement already satisfied: attrs>=18.1.0 in c:\\users\\zeus\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (19.3.0)\n",
      "Collecting promise\n",
      "  Using cached promise-2.3.tar.gz (19 kB)\n",
      "Requirement already satisfied: dill in c:\\users\\zeus\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (0.3.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zeus\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2020.6.20)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\zeus\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\zeus\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\zeus\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (1.25.8)\n",
      "Collecting googleapis-common-protos\n",
      "  Using cached googleapis_common_protos-1.52.0-py2.py3-none-any.whl (100 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\zeus\\anaconda3\\lib\\site-packages (from protobuf>=3.6.1->tensorflow_datasets) (45.2.0.post20200210)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21499 sha256=53682ce121078be7387ae4294dc43895a7d8440676580436ee1a0003cc904a14\n",
      "  Stored in directory: c:\\users\\zeus\\appdata\\local\\pip\\cache\\wheels\\29\\93\\c6\\762e359f8cb6a5b69c72235d798804cae523bbe41c2aa8333d\n",
      "Successfully built promise\n",
      "Installing collected packages: googleapis-common-protos, tensorflow-metadata, promise, tensorflow-datasets\n",
      "Successfully installed googleapis-common-protos-1.52.0 promise-2.3 tensorflow-datasets-3.1.0 tensorflow-metadata-0.22.2\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except:\n",
    "    pass\n",
    "\n",
    "!pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-B-0zxxtwFQV",
    "outputId": "548cd517-3d7c-41de-b6c4-0069a156c0a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â€¢ Using TensorFlow Version: 2.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "print(\"\\u2022 Using TensorFlow Version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JBIRo8Xtl_2h"
   },
   "source": [
    "## Download the IMDB Dataset\n",
    "\n",
    "We will download the [IMDB dataset](https://www.tensorflow.org/datasets/catalog/imdb_reviews) using TensorFlow Datasets. We will use a training set, a validation set, and a test set. Since the IMDB dataset doesn't have a validation split, we will use the first 60\\% of the training set for training, and the last 40\\% of the training set for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "gfnt5ibsl_2i",
    "outputId": "b0ac8d0c-1fcc-4364-c21e-bc8e88b0531e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset imdb_reviews/plain_text/1.0.0 (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to C:\\Users\\Zeus\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0...\u001b[0m\n",
      "Shuffling and writing examples to C:\\Users\\Zeus\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0.incompleteEH8ONB\\imdb_reviews-train.tfrecord\n",
      "Shuffling and writing examples to C:\\Users\\Zeus\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0.incompleteEH8ONB\\imdb_reviews-test.tfrecord\n",
      "Shuffling and writing examples to C:\\Users\\Zeus\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0.incompleteEH8ONB\\imdb_reviews-unsupervised.tfrecord\n",
      "\u001b[1mDataset imdb_reviews downloaded and prepared to C:\\Users\\Zeus\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "splits = ['train[:60%]', 'train[-40%:]', 'test']\n",
    "\n",
    "splits, info = tfds.load(name=\"imdb_reviews\", with_info=True, split=splits, as_supervised=True)\n",
    "\n",
    "train_data, validation_data, test_data = splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PbbsJKch1lL5"
   },
   "source": [
    "## Explore the Data \n",
    "\n",
    "Let's take a moment to look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "3gh4Taekl_2k",
    "outputId": "639d88d7-8d1d-4045-80a6-ad91f0bb98d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dataset has a total of:\n",
      "â€¢ 2 classes\n",
      "â€¢ 25,000 movie reviews for training\n",
      "â€¢ 25,000 movie reviews for testing\n"
     ]
    }
   ],
   "source": [
    "num_train_examples = info.splits['train'].num_examples\n",
    "num_test_examples = info.splits['test'].num_examples\n",
    "num_classes = info.features['label'].num_classes\n",
    "\n",
    "print('The Dataset has a total of:')\n",
    "print('\\u2022 {:,} classes'.format(num_classes))\n",
    "\n",
    "print('\\u2022 {:,} movie reviews for training'.format(num_train_examples))\n",
    "print('\\u2022 {:,} movie reviews for testing'.format(num_test_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SIyvvYFF2DXz"
   },
   "source": [
    "The labels are either 0 or 1, where 0 is a negative review, and 1 is a positive review. We will create a list with the corresponding class names, so that we can map labels to class names later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1SAGQKTdl_2n"
   },
   "outputs": [],
   "source": [
    "class_names = ['negative', 'positive']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wrLs9vQr16JH"
   },
   "source": [
    "Each example consists of a sentence representing the movie review and a corresponding label. The sentence is not preprocessed in any way. Let's take a look at the first example of the training set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "u6lqHTTzl_2q",
    "outputId": "66b6c12c-f35d-41b4-faea-a7bad175e333"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Movie Review:\n",
      "\n",
      " b'This is a big step down after the surprisingly enjoyable original. This sequel isn\\'t nearly as fun as part one, and it instead spends too much time on plot development. Tim Thomerson is still the best thing about this series, but his wisecracking is toned down in this entry. The performances are all adequate, but this time the script lets us down. The action is merely routine and the plot is only mildly interesting, so I need lots of silly laughs in order to stay entertained during a \"Trancers\" movie. Unfortunately, the laughs are few and far between, and so, this film is watchable at best.'\n",
      "\n",
      "Label: negative\n"
     ]
    }
   ],
   "source": [
    "for review, label in train_data.take(1):\n",
    "    review = review.numpy()\n",
    "    label = label.numpy()\n",
    "\n",
    "    print('\\nMovie Review:\\n\\n', review)\n",
    "    print('\\nLabel:', class_names[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Tam__wal_2s"
   },
   "source": [
    "## Load Word Embeddings\n",
    "\n",
    "In this example, the input data consists of sentences. The labels to predict are either 0 or 1.\n",
    "\n",
    "One way to represent the text is to convert sentences into word embeddings. Word embeddings, are an efficient way to represent words using dense vectors, where semantically similar words have similar vectors. We can use a pre-trained text embedding as the first layer of our model, which will have two advantages:\n",
    "\n",
    "*   We don't have to worry anout text preprocessing.\n",
    "*   We can benefit from transfer learning.\n",
    "\n",
    "For this example we will use a model from [TensorFlow Hub](https://tfhub.dev/) called [google/tf2-preview/gnews-swivel-20dim/1](https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1). We'll create a `hub.KerasLayer` that uses the TensorFlow Hub model to embed the sentences. We can choose to fine-tune the TF hub module weights during training by setting the `trainable` parameter to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wbYukNNs2y7H"
   },
   "outputs": [],
   "source": [
    "embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "\n",
    "hub_layer = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P6V9WM7Gl_2v"
   },
   "source": [
    "## Build Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IK1a7HTYl_2w"
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "train_batches = train_data.shuffle(num_train_examples // 4).batch(batch_size).prefetch(1)\n",
    "validation_batches = validation_data.batch(batch_size).prefetch(1)\n",
    "test_batches = test_data.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vJ8uZcqOl_2y"
   },
   "source": [
    "## Build the Model\n",
    "\n",
    "In the code below we will build a Keras `Sequential` model with the following layers:\n",
    "\n",
    "1. The first layer is a TensorFlow Hub layer. This layer uses a pre-trained SavedModel to map a sentence into its embedding vector. The model that we are using ([google/tf2-preview/gnews-swivel-20dim/1](https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1)) splits the sentence into tokens, embeds each token and then combines the embedding. The resulting dimensions are: `(num_examples, embedding_dimension)`.\n",
    "\n",
    "\n",
    "2. This fixed-length output vector is piped through a fully-connected (`Dense`) layer with 16 hidden units.\n",
    "\n",
    "\n",
    "3. The last layer is densely connected with a single output node. Using the `sigmoid` activation function, this value is a float between 0 and 1, representing a probability, or confidence level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2mvUjhw02y9-"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "        hub_layer,\n",
    "        tf.keras.layers.Dense(16, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K-kJSvK57S5F"
   },
   "source": [
    "## Train the Model\n",
    "\n",
    "Since this is a binary classification problem and the model outputs a probability (a single-unit layer with a sigmoid activation), we'll use the `binary_crossentropy` loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "3S8wWDol2zBP",
    "outputId": "bd58dbfd-ddec-47cf-8df7-b8208e818811"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "30/30 [==============================] - 4s 125ms/step - loss: 0.7152 - accuracy: 0.5535 - val_loss: 0.6497 - val_accuracy: 0.6197\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 3s 112ms/step - loss: 0.6258 - accuracy: 0.6532 - val_loss: 0.5988 - val_accuracy: 0.6846\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 3s 105ms/step - loss: 0.5828 - accuracy: 0.7029 - val_loss: 0.5664 - val_accuracy: 0.7236\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 3s 110ms/step - loss: 0.5486 - accuracy: 0.7422 - val_loss: 0.5373 - val_accuracy: 0.7477\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 3s 106ms/step - loss: 0.5152 - accuracy: 0.7687 - val_loss: 0.5082 - val_accuracy: 0.7673\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 3s 113ms/step - loss: 0.4805 - accuracy: 0.7901 - val_loss: 0.4775 - val_accuracy: 0.7857\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 3s 109ms/step - loss: 0.4469 - accuracy: 0.8109 - val_loss: 0.4506 - val_accuracy: 0.7991\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 4s 117ms/step - loss: 0.4161 - accuracy: 0.8263 - val_loss: 0.4269 - val_accuracy: 0.8094\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 3s 107ms/step - loss: 0.3870 - accuracy: 0.8447 - val_loss: 0.4051 - val_accuracy: 0.8190\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 3s 106ms/step - loss: 0.3603 - accuracy: 0.8541 - val_loss: 0.3860 - val_accuracy: 0.8281\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 3s 114ms/step - loss: 0.3347 - accuracy: 0.8684 - val_loss: 0.3689 - val_accuracy: 0.8362\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 3s 109ms/step - loss: 0.3106 - accuracy: 0.8791 - val_loss: 0.3536 - val_accuracy: 0.8443\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 3s 115ms/step - loss: 0.2880 - accuracy: 0.8911 - val_loss: 0.3411 - val_accuracy: 0.8514\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 3s 110ms/step - loss: 0.2677 - accuracy: 0.9015 - val_loss: 0.3294 - val_accuracy: 0.8571\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 3s 110ms/step - loss: 0.2491 - accuracy: 0.9103 - val_loss: 0.3209 - val_accuracy: 0.8614\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 3s 108ms/step - loss: 0.2322 - accuracy: 0.9167 - val_loss: 0.3146 - val_accuracy: 0.8650\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 3s 108ms/step - loss: 0.2164 - accuracy: 0.9241 - val_loss: 0.3090 - val_accuracy: 0.8684\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 3s 109ms/step - loss: 0.2024 - accuracy: 0.9314 - val_loss: 0.3059 - val_accuracy: 0.8692\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 4s 119ms/step - loss: 0.1896 - accuracy: 0.9369 - val_loss: 0.3066 - val_accuracy: 0.8696\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 3s 108ms/step - loss: 0.1774 - accuracy: 0.9416 - val_loss: 0.3040 - val_accuracy: 0.8722\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_batches,\n",
    "                    epochs=20,\n",
    "                    validation_data=validation_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MyORnj-u8F_j"
   },
   "source": [
    "## Evaluate the Model\n",
    "\n",
    "We will now see how well our model performs on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Ch6hq1_kl_23",
    "outputId": "16d47954-2369-4137-9872-f762f79556a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.319\n",
      "accuracy: 0.864\n"
     ]
    }
   ],
   "source": [
    "eval_results = model.evaluate(test_batches, verbose=0)\n",
    "\n",
    "for metric, value in zip(model.metrics_names, eval_results):\n",
    "    print(metric + ': {:.3}'.format(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "text_classification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
