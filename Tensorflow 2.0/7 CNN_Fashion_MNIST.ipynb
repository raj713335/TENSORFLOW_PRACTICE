{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of CNN_Fashion_MNIST.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMY_MVkEVvVk",
        "colab_type": "text"
      },
      "source": [
        "**Convolutional Neural Network Implementation in TensorFlow 2.0**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PW5pXrr6Xl9y",
        "colab_type": "text"
      },
      "source": [
        "![Convolutional Neural Network Architecture](https://miro.medium.com/max/2510/1*vkQ0hXDaQv57sALXAJquxA.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHB_vwWVXrZj",
        "colab_type": "text"
      },
      "source": [
        "**About the dataset**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeYuWEjfe9RM",
        "colab_type": "text"
      },
      "source": [
        "Let us implement a simple convolutional neural network using TensorFlow 2.0. For this, we will make use of the Fashion MNIST dataset by Zalando (MIT License) which contains 70,000 images (in grayscale) in 10 different categories. The images are 28x28 pixels of individual articles of clothing with values ranging from 0 to 255 as shown below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seckBTs0fE95",
        "colab_type": "text"
      },
      "source": [
        "![Fashion MNIST dataset](https://raw.githubusercontent.com/zalandoresearch/fashion-mnist/master/doc/img/fashion-mnist-sprite.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7RC1ya2fXs8",
        "colab_type": "text"
      },
      "source": [
        "Out of the total 70,000 images, 60,000 are used for training and remaining 10,000 for testing. The labels are integer arrays ranging from 0 to 9. The class names are not a part of the dataset and hence we need to include the below mapping while training/prediction:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gH1CpmcSfdHp",
        "colab_type": "text"
      },
      "source": [
        "Label\t-> Description\n",
        "\n",
        "0\t-> T-shirt/top\n",
        "\n",
        "1\t-> Trouser\n",
        "\n",
        "2\t-> Pullover\n",
        "\n",
        "3\t-> Dress\n",
        "\n",
        "4\t-> Coat\n",
        "\n",
        "5\t-> Sandal\n",
        "\n",
        "6\t-> Shirt\n",
        "\n",
        "7\t-> Sneaker\n",
        "\n",
        "8\t-> Bag\n",
        "\n",
        "9\t-> Ankle boot\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56Q51QeJknFy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create class_names list object for mapping labels to names\n",
        "\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnX3qP7aDc9_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the below code to make sure that you select TensorFlow 2.0 in Colab\n",
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIb3JAnDDNng",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "49d77f79-5f8a-4a3b-a0e5-24f2e01e1f53"
      },
      "source": [
        "# Install necessary modules\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "\n",
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as ks\n",
        "\n",
        "# Validating the TensorFlow version\n",
        "print(tf.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LNPDWFrDWyf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "88436f33-7c0c-443d-94d6-40aaa09ee9e6"
      },
      "source": [
        "# Load the Fashion MNIST dataset\n",
        "\n",
        "mnist_fashion = ks.datasets.fashion_mnist\n",
        "\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist_fashion.load_data()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_Fhp6IClLn8",
        "colab_type": "text"
      },
      "source": [
        "**Data Exploration**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhMQlj4yD1Gw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5acc6acc-21ba-4aa8-b2b1-d392cfb5f733"
      },
      "source": [
        "# Shape of Training and Test Set\n",
        "\n",
        "print('Training Dataset Shape: {}'.format(training_images.shape))\n",
        "print('No. of Training Dataset Labels: {}'.format(len(training_labels)))\n",
        "print('Test Dataset Shape: {}'.format(test_images.shape))\n",
        "print('No. of Test Dataset Labels: {}'.format(len(test_labels)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Dataset Shape: (60000, 28, 28)\n",
            "No. of Training Dataset Labels: 60000\n",
            "Test Dataset Shape: (10000, 28, 28)\n",
            "No. of Test Dataset Labels: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckCUKXUbl22y",
        "colab_type": "text"
      },
      "source": [
        "**Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv6UxYCACQ50",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "557e430f-1ca6-4acb-ef8f-efe38b670ab2"
      },
      "source": [
        "training_images[0]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,\n",
              "          0,   0,  13,  73,   0,   0,   1,   4,   0,   0,   0,   0,   1,\n",
              "          1,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "          0,  36, 136, 127,  62,  54,   0,   0,   0,   1,   3,   4,   0,\n",
              "          0,   3],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,\n",
              "          0, 102, 204, 176, 134, 144, 123,  23,   0,   0,   0,   0,  12,\n",
              "         10,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0, 155, 236, 207, 178, 107, 156, 161, 109,  64,  23,  77, 130,\n",
              "         72,  15],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   0,\n",
              "         69, 207, 223, 218, 216, 216, 163, 127, 121, 122, 146, 141,  88,\n",
              "        172,  66],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   1,   1,   0,\n",
              "        200, 232, 232, 233, 229, 223, 223, 215, 213, 164, 127, 123, 196,\n",
              "        229,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        183, 225, 216, 223, 228, 235, 227, 224, 222, 224, 221, 223, 245,\n",
              "        173,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        193, 228, 218, 213, 198, 180, 212, 210, 211, 213, 223, 220, 243,\n",
              "        202,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   3,   0,  12,\n",
              "        219, 220, 212, 218, 192, 169, 227, 208, 218, 224, 212, 226, 197,\n",
              "        209,  52],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,   0,  99,\n",
              "        244, 222, 220, 218, 203, 198, 221, 215, 213, 222, 220, 245, 119,\n",
              "        167,  56],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   4,   0,   0,  55,\n",
              "        236, 228, 230, 228, 240, 232, 213, 218, 223, 234, 217, 217, 209,\n",
              "         92,   0],\n",
              "       [  0,   0,   1,   4,   6,   7,   2,   0,   0,   0,   0,   0, 237,\n",
              "        226, 217, 223, 222, 219, 222, 221, 216, 223, 229, 215, 218, 255,\n",
              "         77,   0],\n",
              "       [  0,   3,   0,   0,   0,   0,   0,   0,   0,  62, 145, 204, 228,\n",
              "        207, 213, 221, 218, 208, 211, 218, 224, 223, 219, 215, 224, 244,\n",
              "        159,   0],\n",
              "       [  0,   0,   0,   0,  18,  44,  82, 107, 189, 228, 220, 222, 217,\n",
              "        226, 200, 205, 211, 230, 224, 234, 176, 188, 250, 248, 233, 238,\n",
              "        215,   0],\n",
              "       [  0,  57, 187, 208, 224, 221, 224, 208, 204, 214, 208, 209, 200,\n",
              "        159, 245, 193, 206, 223, 255, 255, 221, 234, 221, 211, 220, 232,\n",
              "        246,   0],\n",
              "       [  3, 202, 228, 224, 221, 211, 211, 214, 205, 205, 205, 220, 240,\n",
              "         80, 150, 255, 229, 221, 188, 154, 191, 210, 204, 209, 222, 228,\n",
              "        225,   0],\n",
              "       [ 98, 233, 198, 210, 222, 229, 229, 234, 249, 220, 194, 215, 217,\n",
              "        241,  65,  73, 106, 117, 168, 219, 221, 215, 217, 223, 223, 224,\n",
              "        229,  29],\n",
              "       [ 75, 204, 212, 204, 193, 205, 211, 225, 216, 185, 197, 206, 198,\n",
              "        213, 240, 195, 227, 245, 239, 223, 218, 212, 209, 222, 220, 221,\n",
              "        230,  67],\n",
              "       [ 48, 203, 183, 194, 213, 197, 185, 190, 194, 192, 202, 214, 219,\n",
              "        221, 220, 236, 225, 216, 199, 206, 186, 181, 177, 172, 181, 205,\n",
              "        206, 115],\n",
              "       [  0, 122, 219, 193, 179, 171, 183, 196, 204, 210, 213, 207, 211,\n",
              "        210, 200, 196, 194, 191, 195, 191, 198, 192, 176, 156, 167, 177,\n",
              "        210,  92],\n",
              "       [  0,   0,  74, 189, 212, 191, 175, 172, 175, 181, 185, 188, 189,\n",
              "        188, 193, 198, 204, 209, 210, 210, 211, 188, 188, 194, 192, 216,\n",
              "        170,   0],\n",
              "       [  2,   0,   0,   0,  66, 200, 222, 237, 239, 242, 246, 243, 244,\n",
              "        221, 220, 193, 191, 179, 182, 182, 181, 176, 166, 168,  99,  58,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  40,  61,  44,  72,  41,  35,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTt1pqz0l6tp",
        "colab_type": "text"
      },
      "source": [
        "As the pixel values range from 0 to 255, we have to scale these values to a range of 0 to 1 before feeding them to the model. We can scale these values (both for training and test datasets) by dividing the values by 255:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMZ1yl8FEI_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_images = training_images / 255.0\n",
        "\n",
        "test_images = test_images / 255.0"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DgRCQuVuI3r",
        "colab_type": "text"
      },
      "source": [
        "Reshaping the Training and Test dataset by reshaping the matrices into 28x28x1 array: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzyJ3LzLI2dK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_images = training_images.reshape((60000, 28, 28, 1))\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "era6idKR2a08",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "4a3ad50e-6eb2-4083-9c1b-1199bfaefe2c"
      },
      "source": [
        "# Shape of Training and Test Set after applying reshape\n",
        "\n",
        "print('Training Dataset Shape: {}'.format(training_images.shape))\n",
        "print('No. of Training Dataset Labels: {}'.format(len(training_labels)))\n",
        "print('Test Dataset Shape: {}'.format(test_images.shape))\n",
        "print('No. of Test Dataset Labels: {}'.format(len(test_labels)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Dataset Shape: (60000, 28, 28, 1)\n",
            "No. of Training Dataset Labels: 60000\n",
            "Test Dataset Shape: (10000, 28, 28, 1)\n",
            "No. of Test Dataset Labels: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miCPDg1uvD8z",
        "colab_type": "text"
      },
      "source": [
        "**Model Building**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKp_6degvwnX",
        "colab_type": "text"
      },
      "source": [
        "We will be using the keras implementation to build the different layers of a CNN. We will keep it simple by having only 2 layers. \n",
        "\n",
        "**First Layer - Convolutional layer with ReLU activation function:** This layer takes the 2D array (28x28 pixels) as input. We will take 50 convolutional kernels (filters) of shape 3x3 pixels, output of whose will be passed to a ReLU activation function before it is passed to the next layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMLYMBi8EQUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn_model = ks.models.Sequential()\n",
        "cnn_model.add(ks.layers.Conv2D(40, (3, 3), activation='relu', input_shape=(28, 28, 1), name='Convolutional_layer'))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToKE6u4p0ZuM",
        "colab_type": "text"
      },
      "source": [
        "**Second Layer - Pooling layer:** This layer takes the 50 26x26 2D arrays as input and transforms them into the same number (50) of arrays with dimensions half of that of the original (i.e. from 26x26 to 13x13 pixels)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_iucfrP0YsY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn_model.add(ks.layers.MaxPooling2D((2, 2), name='Maxpooling_2D'))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8F9uYCt9irik",
        "colab_type": "text"
      },
      "source": [
        "**Third Layer - Fully Connected layer:** This layer takes the 50 13x13 2D arrays as input and transforms them into a 1D array of 8450 elements (50x13x13). These 8450 input elements are passed through a fully connected neural network which gives out the probability scores for each of the 10 output labels (at the output layer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FS9L_8kTFRz4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn_model.add(ks.layers.Flatten(name='Flatten'))\n",
        "cnn_model.add(ks.layers.Dense(50, activation='relu', name='Hidden_layer'))\n",
        "cnn_model.add(ks.layers.Dense(10, activation='softmax', name='Output_layer'))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBL5QRdJj8yG",
        "colab_type": "text"
      },
      "source": [
        "We can check the details of different layers built in the CNN model by using the summary method as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Paa3H8AmFYjP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "a1f2a622-c317-4a9d-a5ca-d2ee134712a3"
      },
      "source": [
        "cnn_model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "Convolutional_layer (Conv2D) (None, 26, 26, 40)        400       \n",
            "_________________________________________________________________\n",
            "Maxpooling_2D (MaxPooling2D) (None, 13, 13, 40)        0         \n",
            "_________________________________________________________________\n",
            "Flatten (Flatten)            (None, 6760)              0         \n",
            "_________________________________________________________________\n",
            "Hidden_layer (Dense)         (None, 50)                338050    \n",
            "_________________________________________________________________\n",
            "Output_layer (Dense)         (None, 10)                510       \n",
            "=================================================================\n",
            "Total params: 338,960\n",
            "Trainable params: 338,960\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tcp73o-7lkfo",
        "colab_type": "text"
      },
      "source": [
        "Now, we will use an optimization function with the help of compile method. An Adam optimizer with objective function as sparse_categorical_crossentropy which optimzes for the accuracy metric can be built as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_fyxXveFagh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISBAFVg5mYtz",
        "colab_type": "text"
      },
      "source": [
        "**Model Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_irVQYjSmX7G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "970d08d2-0b35-4ea8-d18d-aafdabb3152f"
      },
      "source": [
        "cnn_model.fit(training_images, training_labels, epochs=10)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 35s 19ms/step - loss: 0.4162 - accuracy: 0.8537\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 35s 19ms/step - loss: 0.2824 - accuracy: 0.8990\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 35s 18ms/step - loss: 0.2409 - accuracy: 0.9118\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 35s 18ms/step - loss: 0.2136 - accuracy: 0.9223\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 34s 18ms/step - loss: 0.1911 - accuracy: 0.9293\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 34s 18ms/step - loss: 0.1693 - accuracy: 0.9371\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 34s 18ms/step - loss: 0.1527 - accuracy: 0.9440\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 34s 18ms/step - loss: 0.1370 - accuracy: 0.9498\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 34s 18ms/step - loss: 0.1218 - accuracy: 0.9553\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 34s 18ms/step - loss: 0.1093 - accuracy: 0.9598\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3962fb3be0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXA1brtrmgdI",
        "colab_type": "text"
      },
      "source": [
        "**Model Evaluation**\n",
        "\n",
        "1. Training Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRISUog8RNJF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ee4bc8a6-7808-4a68-e8c1-0421ee238a2f"
      },
      "source": [
        "training_loss, training_accuracy = cnn_model.evaluate(training_images, training_labels)\n",
        "\n",
        "print('Training Accuracy {}'.format(round(float(training_accuracy), 2)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0862 - accuracy: 0.9703\n",
            "Training Accuracy 0.97\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sL_6AuJomqel",
        "colab_type": "text"
      },
      "source": [
        "2. Test Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KNIAUSEFgPO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f59300fc-375e-47f5-fd67-407d78983255"
      },
      "source": [
        "test_loss, test_accuracy = cnn_model.evaluate(test_images, test_labels)\n",
        "\n",
        "print('Test Accuracy {}'.format(round(float(test_accuracy), 2)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 2s 8ms/step - loss: 0.2845 - accuracy: 0.9132\n",
            "Test Accuracy 0.91\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do51fxgtnOh7",
        "colab_type": "text"
      },
      "source": [
        "From the above evaluation, we see that we were able to achieve around 97% accuracy in Training dataset and around 91% accuracy in Test dataset just with a simple CNN architecture. This goes on to prove that CNNs are powerful algorithms for Image recognition."
      ]
    }
  ]
}